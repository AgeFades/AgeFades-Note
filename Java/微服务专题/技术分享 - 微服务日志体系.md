[TOC]

# 技术分享 - 微服务日志体系

## 问题背景

- 随着微服务项目架构技术兴起，项目随业务拆分成n多个微服务小项目
- 每个微服务都是独立的JVM进程，运行日志从原来的集中单个文件收集，变成零零散散的位于不同服务器下、不同实例下
- 举例：
  - 项目在产线中，每个微服务部署了三个实例做负载提供功能
  - 现在产线出现bug，当天创建订单失败，客户端只能拿到500系统异常的提示，第二天开发收到Bug，开始排查
  - 代码中，请求进入依次经过：
    1. 网关
    2. 授权服务
    3. 用户服务
    4. 订单服务
    5. 仓储服务
    6. 积分服务
  - 由上可知，需依次排查6个微服务，每个微服务有三个实例在不同的机器上，最坏的情况下，开发就需要去18台机器上看昨天压缩的日志，压缩日志可能还有几份
  - 总结：微服务下日志的零散分布，会导致问题排查的难度大大提升，不利于快速定位问题、修复问题

### 问题示例图

[微服务下零散日志示例](https://www.processon.com/view/link/61aefbcde0b34d02d8ba2d7f)

## 问题分析

1. 日志的统一收集
2. 日志的统一存储
3. 日志的统一展示、查询，友好客户端

## 技术选型

[ELK](https://www.elastic.co/cn/what-is/elk-stack)

[Graylog](https://www.graylog.org/)

- 根据市面使用率，选择ELK
  - 成熟的社区
  - 活跃的版本更新、功能升级、Bug修复
  - 相关博文文档资料丰富

## 项目实施

### 主要使用技术框架概览

- SpringBoot + SpringCloud Netflix 部分组件 + Spring Cloud Alibaba Nacos
- Elasticsearch + Logstash + Kibana
- Sentry（异常实时监控报警）
  - [搭建部署简单使用文档](https://github.com/AgeFades/AgeFades-Note/blob/master/%E5%B7%A5%E5%85%B7/Sentry.md)
- MySQL + Redis

### ELK部署

- 使用 docker-compose 编排部署
- 前提：安装 docker、docker-compose

#### 准备挂载目录

```bash
# 创建挂载目录
mkdir es-conf es-data es-logs es-plugins kibana-conf logstash-conf logstash-logs

# 修改相关目录权限
chown -R 1000:0 es-data && chown -R 1000:0 es-logs && chown -R 1000:0 es-conf && chown -R 1000:0 es-plugins

# 修改相关系统资源设定
sysctl -w vm.max_map_count=655360
ulimit -u 65535
sysctl -p
```

#### 编辑es配置文件

```yml
# vim es-conf/elasticsearch.yml

# 集群名
cluster.name: es-cluster
# 当前节点名
node.name: es-server
# 网络绑定host
network.bind_host: 0.0.0.0
# host名称
network.publish_host: elasticsearch
# http通信端口
http.port: 9200
# tcp通信端口
transport.tcp.port: 9300
# 开启跨域配置
http.cors.enabled: true
http.cors.allow-origin: "*"
# 当前节点为master节点
node.master: true
# 当前节点为数据节点
node.data: true
# 服务发现ping ip:port
discovery.zen.ping.unicast.hosts: ["elasticsearch:9300"]
# 最小master节点数
discovery.zen.minimum_master_nodes: 1
# 单节点类型
discovery.type: single-node
# 新增如下xpack配置开启安全认证
# xpack.security.enabled: true
# 设置为基础，其他级别需要购买
# xpack.license.self_generated.type: basic
# 开启transport端口ssl认证
# xpack.security.transport.ssl.enabled: true
```

```properties
# vim es-conf/log4j2.properties

status = error

######## Server JSON ############################
appender.rolling.type = Console
appender.rolling.name = rolling
appender.rolling.layout.type = ESJsonLayout
appender.rolling.layout.type_name = server

################################################

################################################

rootLogger.level = info
rootLogger.appenderRef.rolling.ref = rolling

######## Deprecation JSON #######################
appender.deprecation_rolling.type = Console
appender.deprecation_rolling.name = deprecation_rolling
appender.deprecation_rolling.layout.type = ESJsonLayout
appender.deprecation_rolling.layout.type_name = deprecation
appender.deprecation_rolling.layout.esmessagefields=x-opaque-id
appender.deprecation_rolling.filter.rate_limit.type = RateLimitingFilter

appender.header_warning.type = HeaderWarningAppender
appender.header_warning.name = header_warning
#################################################

#################################################
logger.deprecation.name = org.elasticsearch.deprecation
logger.deprecation.level = deprecation
logger.deprecation.appenderRef.deprecation_rolling.ref = deprecation_rolling
logger.deprecation.appenderRef.header_warning.ref = header_warning
logger.deprecation.additivity = false

######## Search slowlog JSON ####################
appender.index_search_slowlog_rolling.type = Console
appender.index_search_slowlog_rolling.name = index_search_slowlog_rolling
appender.index_search_slowlog_rolling.layout.type = ESJsonLayout
appender.index_search_slowlog_rolling.layout.type_name = index_search_slowlog
appender.index_search_slowlog_rolling.layout.esmessagefields=message,took,took_millis,total_hits,types,stats,search_type,total_shards,source,id

#################################################

#################################################
logger.index_search_slowlog_rolling.name = index.search.slowlog
logger.index_search_slowlog_rolling.level = trace
logger.index_search_slowlog_rolling.appenderRef.index_search_slowlog_rolling.ref = index_search_slowlog_rolling
logger.index_search_slowlog_rolling.additivity = false

######## Indexing slowlog JSON ##################
appender.index_indexing_slowlog_rolling.type = Console
appender.index_indexing_slowlog_rolling.name = index_indexing_slowlog_rolling
appender.index_indexing_slowlog_rolling.layout.type = ESJsonLayout
appender.index_indexing_slowlog_rolling.layout.type_name = index_indexing_slowlog
appender.index_indexing_slowlog_rolling.layout.esmessagefields=message,took,took_millis,doc_type,id,routing,source

#################################################

#################################################

logger.index_indexing_slowlog.name = index.indexing.slowlog.index
logger.index_indexing_slowlog.level = trace
logger.index_indexing_slowlog.appenderRef.index_indexing_slowlog_rolling.ref = index_indexing_slowlog_rolling
logger.index_indexing_slowlog.additivity = false

appender.audit_rolling.type = Console
appender.audit_rolling.name = audit_rolling
appender.audit_rolling.layout.type = PatternLayout
appender.audit_rolling.layout.pattern = {\
                "type":"audit", \
                "timestamp":"%d{yyyy-MM-dd'T'HH:mm:ss,SSSZ}"\
                %varsNotEmpty{, "node.name":"%enc{%map{node.name}}{JSON}"}\
                %varsNotEmpty{, "node.id":"%enc{%map{node.id}}{JSON}"}\
                %varsNotEmpty{, "host.name":"%enc{%map{host.name}}{JSON}"}\
                %varsNotEmpty{, "host.ip":"%enc{%map{host.ip}}{JSON}"}\
                %varsNotEmpty{, "event.type":"%enc{%map{event.type}}{JSON}"}\
                %varsNotEmpty{, "event.action":"%enc{%map{event.action}}{JSON}"}\
                %varsNotEmpty{, "authentication.type":"%enc{%map{authentication.type}}{JSON}"}\
                %varsNotEmpty{, "user.name":"%enc{%map{user.name}}{JSON}"}\
                %varsNotEmpty{, "user.run_by.name":"%enc{%map{user.run_by.name}}{JSON}"}\
                %varsNotEmpty{, "user.run_as.name":"%enc{%map{user.run_as.name}}{JSON}"}\
                %varsNotEmpty{, "user.realm":"%enc{%map{user.realm}}{JSON}"}\
                %varsNotEmpty{, "user.run_by.realm":"%enc{%map{user.run_by.realm}}{JSON}"}\
                %varsNotEmpty{, "user.run_as.realm":"%enc{%map{user.run_as.realm}}{JSON}"}\
                %varsNotEmpty{, "user.roles":%map{user.roles}}\
                %varsNotEmpty{, "apikey.id":"%enc{%map{apikey.id}}{JSON}"}\
                %varsNotEmpty{, "apikey.name":"%enc{%map{apikey.name}}{JSON}"}\
                %varsNotEmpty{, "origin.type":"%enc{%map{origin.type}}{JSON}"}\
                %varsNotEmpty{, "origin.address":"%enc{%map{origin.address}}{JSON}"}\
                %varsNotEmpty{, "realm":"%enc{%map{realm}}{JSON}"}\
                %varsNotEmpty{, "url.path":"%enc{%map{url.path}}{JSON}"}\
                %varsNotEmpty{, "url.query":"%enc{%map{url.query}}{JSON}"}\
                %varsNotEmpty{, "request.method":"%enc{%map{request.method}}{JSON}"}\
                %varsNotEmpty{, "request.body":"%enc{%map{request.body}}{JSON}"}\
                %varsNotEmpty{, "request.id":"%enc{%map{request.id}}{JSON}"}\
                %varsNotEmpty{, "action":"%enc{%map{action}}{JSON}"}\
                %varsNotEmpty{, "request.name":"%enc{%map{request.name}}{JSON}"}\
                %varsNotEmpty{, "indices":%map{indices}}\
                %varsNotEmpty{, "opaque_id":"%enc{%map{opaque_id}}{JSON}"}\
                %varsNotEmpty{, "x_forwarded_for":"%enc{%map{x_forwarded_for}}{JSON}"}\
                %varsNotEmpty{, "transport.profile":"%enc{%map{transport.profile}}{JSON}"}\
                %varsNotEmpty{, "rule":"%enc{%map{rule}}{JSON}"}\
                %varsNotEmpty{, "put":%map{put}}\
                %varsNotEmpty{, "delete":%map{delete}}\
                %varsNotEmpty{, "change":%map{change}}\
                %varsNotEmpty{, "create":%map{create}}\
                %varsNotEmpty{, "invalidate":%map{invalidate}}\
                }%n

logger.xpack_security_audit_logfile.name = org.elasticsearch.xpack.security.audit.logfile.LoggingAuditTrail
logger.xpack_security_audit_logfile.level = info
logger.xpack_security_audit_logfile.appenderRef.audit_rolling.ref = audit_rolling
logger.xpack_security_audit_logfile.additivity = false

logger.xmlsig.name = org.apache.xml.security.signature.XMLSignature
logger.xmlsig.level = error
logger.samlxml_decrypt.name = org.opensaml.xmlsec.encryption.support.Decrypter
logger.samlxml_decrypt.level = fatal
logger.saml2_decrypt.name = org.opensaml.saml.saml2.encryption.Decrypter
logger.saml2_decrypt.level = fatal
```

```apl
# vim es-conf/jvm.options

## GC configuration
8-13:-XX:+UseConcMarkSweepGC
8-13:-XX:CMSInitiatingOccupancyFraction=75
8-13:-XX:+UseCMSInitiatingOccupancyOnly

## G1GC Configuration
# NOTE: G1 GC is only supported on JDK version 10 or later
# to use G1GC, uncomment the next two lines and update the version on the
# following three lines to your version of the JDK
# 10-13:-XX:-UseConcMarkSweepGC
# 10-13:-XX:-UseCMSInitiatingOccupancyOnly
14-:-XX:+UseG1GC

## JVM temporary directory
-Djava.io.tmpdir=${ES_TMPDIR}

## heap dumps

# generate a heap dump when an allocation from the Java heap fails; heap dumps
# are created in the working directory of the JVM unless an alternative path is
# specified
-XX:+HeapDumpOnOutOfMemoryError

# specify an alternative path for heap dumps; ensure the directory exists and
# has sufficient space
-XX:HeapDumpPath=data

# specify an alternative path for JVM fatal error logs
-XX:ErrorFile=logs/hs_err_pid%p.log

## JDK 8 GC logging
8:-XX:+PrintGCDetails
8:-XX:+PrintGCDateStamps
8:-XX:+PrintTenuringDistribution
8:-XX:+PrintGCApplicationStoppedTime
8:-Xloggc:logs/gc.log
8:-XX:+UseGCLogFileRotation
8:-XX:NumberOfGCLogFiles=32
8:-XX:GCLogFileSize=64m

# JDK 9+ GC logging
9-:-Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m
```

#### 编辑kibana配置文件

```yaml
# vim kibana-conf/kibana.yml

server.name: kibana
server.host: "0"
elasticsearch.hosts: [ "http://elasticsearch:9200" ]
xpack.monitoring.ui.container.elasticsearch.enabled: true
# 连接 es 的账号密码
elasticsearch.username: "elastic"
elasticsearch.password: "elastic"
i18n.locale: "zh-CN"
```

#### 编辑logstash配置文件

```yaml
# vim logstash-conf/logstash.yml

path.config: /usr/share/logstash/conf.d/*.conf
path.logs: /var/log/logstash
# 绑定的主机地址
http.host: 0.0.0.0
# 输出通道的工作workers数据量（提升输出效率）,建议修改为CPU核数
pipeline.workers: 4
```

```apl
# vim logstash-conf/conf.d/tcp-log.conf

input {
    tcp {
     port => 9400
      codec => json {
             charset => "UTF-8"
      }
    }
    stdin{}
}
filter {
}

output {
  elasticsearch {
    action => "index"
    #这里是es的地址，多个es要写成数组的形式
    hosts  => "elasticsearch:9200"
    #用于kibana过滤，可以填项目名称
    index  => "test-log-%{+YYYY-MM-dd}"
    user => elastic
    password => elastic
    timeout => 300
  }
}
```

#### 编辑docker-compose文件

```yaml
# vim docker-compose-elk.yaml

version: '3.4'
services:
  elasticsearch:
    container_name: elasticsearch      # 指定容器的名称
    image: docker.io/elasticsearch:7.11.2        # 指定镜像和版本
    restart: always  # 自动重启
    hostname: elasticsearch
    ports:
      - 9200:9200
      - 9300:9300
    environment:
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
      TZ: Asia/Shanghai
      LANG: en_US.UTF-8
      ELASTIC_PASSWORD: "elastic" # elastic账号密码
    privileged: true
    volumes: # 挂载目录
      - ./es-data:/usr/share/elasticsearch/data
      - ./es-plugins:/usr/share/elasticsearch/plugins
      - ./es-conf:/usr/share/elasticsearch/config
      - ./es-logs:/usr/share/elasticsearch/logs

  kibana:
    container_name: kibana       # 指定容器的名称
    image: docker.io/kibana:7.11.2                                 # 指定镜像和版本
    restart: always  # 自动重启
    hostname: kibana
    ports:
      - 5601:5601
    depends_on:
      - elasticsearch
    environment:
      ELASTICSEARCH_URL: "http://elasticsearch:9200"
    privileged: true
    volumes: # 挂载目录
      - ./kibana-conf/kibana.yml:/usr/share/kibana/config/kibana.yml

  logstash:
    container_name: logstash
    image: docker.io/logstash:7.11.2
    restart: always
    hostname: logstash
    ports:
      - 5044:5044
      - 9600:9600
      - 9400:9400
    privileged: true
    volumes:
      - ./logstash-conf/logstash.yml:/usr/share/logstash/config/logstash.yml
      - ./logstash-conf/conf.d:/usr/share/logstash/conf.d
      - ./logstash-logs:/var/log/logstash
```

#### 编辑简单部署脚本

```bash
# vim deploy.sh

docker-compose -f docker-compose-elk.yaml down
docker-compose -f docker-compose-elk.yaml up -d
```

#### 启动ELK容器

```bash
# 执行部署脚本即可
sh deploy.sh
```

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638859549955.png)

#### 默认账号密码

- 默认账号：elastic
- 默认密码：123456
  - 在上面的 compose 文件中，已通过环境变量修改了默认密码

### SpringBoot集成Logstash

#### pom依赖

- 这里 logstash-logback.version 版本用的6.1

```xml
<!-- logstash-logback -->
<dependency>
  <groupId>net.logstash.logback</groupId>
  <artifactId>logstash-logback-encoder</artifactId>
  <version>${logstash-logback.version}</version>
</dependency>
```

#### logback配置

- 完整举例如下

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration scan="true" scanPeriod="60 seconds">

    <!-- 彩色日志依赖的渲染类 -->
    <conversionRule conversionWord="clr" converterClass="org.springframework.boot.logging.logback.ColorConverter" />
    <conversionRule conversionWord="wex" converterClass="org.springframework.boot.logging.logback.WhitespaceThrowableProxyConverter" />
    <conversionRule conversionWord="wEx" converterClass="org.springframework.boot.logging.logback.ExtendedWhitespaceThrowableProxyConverter" />
    <!-- 彩色日志格式 -->
    <property name="CONSOLE_LOG_PATTERN" value="%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr([%-5level]) %clr([%X{userId}]){faint} %clr([%X{traceId}]){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}%L:){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}" />

    <springProperty scope="context" name="log.path" source="logging.file.path"/>
    <springProperty scope="context" name="app.name" source="spring.application.name"/>
    <springProperty scope="context" name="app.profile" source="spring.profiles.active"/>

    <!-- 控制台输出 -->
    <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern>${CONSOLE_LOG_PATTERN}</pattern>
            <charset>utf8</charset>
        </encoder>
    </appender>

    <!-- 生成日志文件 -->
    <appender name="file" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 日志日常打印文件 -->
        <file>./logs/${app.name}.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!-- 日志文件输出的文件名 -->
            <FileNamePattern>./logs/${app.name}/app-%d{yyyy-MM-dd}.%i.log.gz</FileNamePattern>
            <!-- 日志文件保留天数 -->
            <MaxHistory>15</MaxHistory>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>50MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%-5level] [%X{userId}] [%X{traceId}] [%15.15t] %-40.40logger{39}%L: %m%n</pattern>
        </encoder>
    </appender>

    <appender name="logstash" class="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>127.0.0.1:9400</destination>
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <timestamp>
                    <timeZone>UTC</timeZone>
                </timestamp>
                <pattern>
                    <pattern>
                        {
                        <!--设置项目-->
                        "app": "${app.name:-}",
                        <!--设置环境-->
                        "profile": "${app.profile:-}",
                        <!--设置等级-->
                        "level": "%level",
                        <!--设置userId-->
                        "userId": "%X{userId}",
                        <!--设置traceId-->
                        "traceId": "%X{traceId}",
                        <!--设置消息-->
                        "message": "[%thread] [%logger{35}:%L] %msg"
                        }
                    </pattern>
                </pattern>
            </providers>
        </encoder>
    </appender>

    <!-- 日志输出级别 -->
    <root level="INFO">
        <appender-ref ref="console" />
        <appender-ref ref="file" />
        <appender-ref ref="logstash"/>
    </root>

    <!-- 减少nacos日志 -->
    <logger name="com.alibaba.nacos" level="ERROR"/>

</configuration>
```

- 集成完毕，启动测试能否推送即可
- 对业务代码0侵入

### Kibana的简单设置

#### 修改时间显示格式化

​		打开菜单  -->  打开Stack Management -->  找到Kibana  -->  选择高级设置  -->  修改日期格式

​		![](https://blog-kang.oss-cn-beijing.aliyuncs.com/1619172621747.png)

#### 新建日志视图

​		打开菜单  -->  打开Stack Management -->  找到Kibana  -->  选择索引模式  -->  点击创建索引模式

![](https://blog-kang.oss-cn-beijing.aliyuncs.com/1619172816896.png)

​		点击创建编辑如下输入日志表达式，匹配格式，匹配日志索引，点击下一步

![](https://blog-kang.oss-cn-beijing.aliyuncs.com/1619172878418.png)

​		选择时间字段，TCP方式Logstash推送日志后会自动生成，选择日志即可，点击创建索引模式

![](https://blog-kang.oss-cn-beijing.aliyuncs.com/image-20210423181557486.png)

#### 使用日志视图

​		打开菜单  -->  打开Discover -->  点击进入

![](https://blog-kang.oss-cn-beijing.aliyuncs.com/1619173031562.png)

​			打开菜单  -->  打开Discover -->  点击下拉框选择创建的索引匹配器，即可打开日志视图

![](https://blog-kang.oss-cn-beijing.aliyuncs.com/1619173136504.png)

#### 日志筛选

​		点击添加筛选，选择运算符是 （等于），设置日志级别即可过滤

![](https://blog-kang.oss-cn-beijing.aliyuncs.com/1619173244068.png)

#### 查看指定信息

​		点击Message，点击添加Message，就只查看Message的信息

​		![](https://blog-kang.oss-cn-beijing.aliyuncs.com/1619173334405.png)

​				点击点击后查看的结果如下

![](https://blog-kang.oss-cn-beijing.aliyuncs.com/1619173387005.png)

- 更多的筛选语法请自行搜索相关资料

### 简单服务间调用案例展示

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638860386539.png)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638860558182.png)

### 简单异常报警案例展示

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638872096590.png)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638872135737.png)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638872279372.png)

## 技术细节

### 统一返回类的设定

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638872743400.png)

### traceId的生成与传递

#### 公共过滤器

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638872914740.png)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638873297525.png)

#### 父子线程传递

- 在Spring中，通常使用 @Asycn 标记注解来开启一个异步线程执行任务

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638873039858.png)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638873121853.png)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638873244753.png)

#### 服务调用传递

[Feign的使用与常见配置](https://github.com/AgeFades/AgeFades-Note/blob/master/Java/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%93%E9%A2%98/Fox%20-%20Feign.md)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638873385787.png)

### 系统异常设计约定

#### 响应码设计约定

- 所有的500均应交由后端修复

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638873477223.png)

#### 全局异常处理

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638873631907.png)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638873671027.png)

#### 统一断言业务校验

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638873722791.png)

##### 使用案例

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638873767405.png)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638873801480.png)

#### 特殊Code枚举响应

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638873872263.png)

##### 使用案例

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638873907780.png)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638873941422.png)

### 统一日志切面与Http交互日志

#### 日志切面

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638874350659.png)

#### 接口超时报警

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638874416684.png)

#### Http三方交互日志

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638874585936.png)

### RBAC权限模型+认证鉴权流程

[文档地址](https://github.com/AgeFades/AgeFades-Note/blob/master/%E5%B7%A5%E5%85%B7/%E8%AE%A4%E8%AF%81%E9%89%B4%E6%9D%83%E6%B5%81%E7%A8%8B.md)

- 根据以往经验，曾尝试使用Spring Security、Shiro、OAuth2 等相关框架，结合 学习成本、维护成本、可扩展性、轻便简洁性，最终决定自己写该流程。

#### 登陆发放token、缓存信息

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638931598055.png)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638931617784.png)

#### 忽略认证鉴权接口配置

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638930595864.png)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638930660194.png)

#### 认证鉴权过滤器

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638930719929.png)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638931351024.png)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638931416134.png)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638931444718.png)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1638931479414.png)

## Skywalking链路追踪

[官方文档](https://skywalking.apache.org/)

[中文文档社区](https://skyapm.github.io/)

### 实践设计

- 接口超时报警时，携带 skywalking tid，到 skywalking ui 中 `追踪` 根据 tid 过滤出请求链路，分析耗时原因

### 安装部署

- 在上文中的 ELK docker-compose 文件，添加 skywalking 相关组件

```yaml
version: '3.4'
services:
  elasticsearch:
    container_name: elasticsearch      # 指定容器的名称
    image: docker.io/elasticsearch:7.11.2        # 指定镜像和版本
    restart: always  # 自动重启
    hostname: elasticsearch
    ports:
      - 9200:9200
      - 9300:9300
    environment:
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
      TZ: Asia/Shanghai
      LANG: en_US.UTF-8
      ELASTIC_PASSWORD: "elastic" # elastic账号密码
    privileged: true
    volumes: # 挂载目录
      - ./es-data:/usr/share/elasticsearch/data
      - ./es-plugins:/usr/share/elasticsearch/plugins
      - ./es-conf:/usr/share/elasticsearch/config
      - ./es-logs:/usr/share/elasticsearch/logs

  kibana:
    container_name: kibana       # 指定容器的名称
    image: docker.io/kibana:7.11.2                                 # 指定镜像和版本
    restart: always  # 自动重启
    hostname: kibana
    ports:
      - 5601:5601
    depends_on:
      - elasticsearch
    environment:
      ELASTICSEARCH_URL: "http://elasticsearch:9200"
    privileged: true
    volumes: # 挂载目录
      - ./kibana-conf/kibana.yml:/usr/share/kibana/config/kibana.yml

  logstash:
    container_name: logstash
    image: docker.io/logstash:7.11.2
    restart: always
    hostname: logstash
    ports:
      - 5044:5044
      - 9600:9600
      - 9400:9400
    privileged: true
    volumes:
      - ./logstash-conf/logstash.yml:/usr/share/logstash/config/logstash.yml
      - ./logstash-conf/conf.d:/usr/share/logstash/conf.d
      - ./logstash-logs:/var/log/logstash

  skyWalking-server:
    container_name: skyWalking-server       # 指定容器的名称
    image: apache/skywalking-oap-server:8.7.0-es7        # 指定镜像和版本
    restart: always  # 自动重启
    hostname: skyWalking-server         # 主机名
    ports:
      - 11800:11800
      - 12800:12800
    depends_on:
      - elasticsearch
    environment:
      SW_STORAGE: "elasticsearch7"
      SW_STORAGE_ES_CLUSTER_NODES: "elasticsearch:9200"
      SW_ES_PASSWORD: "elastic"
      SW_ES_PASSWORD: "elastic"
    privileged: true

  skyWalking-ui:
    container_name: skyWalking-ui       # 指定容器的名称
    image: apache/skywalking-ui:8.7.0        # 指定镜像和版本
    restart: always  # 自动重启
    hostname: skyWalking-ui         # 主机名
    ports:
      - 8080:8080
    environment:
      SW_OAP_ADDRESS: "http://skyWalking-server:12800"
    privileged: true
```

- sh deploy.sh 重启即可

- Web UI 访问地址默认为：localhost:8080

### Agent设置

[Agent Jar下载地址](https://skywalking.apache.org/downloads/)

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1639039580298.png)

#### 项目Dockerfile集成Skywalking Agent

[知乎 - 参考文档](https://zhuanlan.zhihu.com/p/361103355)

#### Jar包启动方式

```bash
# 注意 skywalking jar包地址
# 注意 skywalking 服务 ip + port
# 注意 项目名称、jar包名
java -javaagent:./skywalking-agent.jar 
		 -Dskywalking.collector.backend_service=localhost:11800 
		 -Dskywalking.agent.service_name=demo -jar app.jar
```

#### IDEA启动方式

- 首先打开Boot，选择Edit Config


![](https://blog-kang.oss-cn-beijing.aliyuncs.com/1638946213943.png)

- 同样在VM options中配置启动的Vm参数

![](https://blog-kang.oss-cn-beijing.aliyuncs.com/1638946267338.png)

```apl
-javaagent:/Users/bigkang/Documents/test/skywalking-agent/skywalking-agent.jar -Dskywalking.collector.backend_service=localhost:11800 -Dskywalking.agent.service_name=demo
```

### 添加依赖

- 这里用的8.7.0版本

```xml
<!-- skywalking -->
<dependency>
  <groupId>org.apache.skywalking</groupId>
  <artifactId>apm-toolkit-trace</artifactId>
  <version>${skywalking.version}</version>
</dependency>
```

### 接口超时报警加tid

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1639039962553.png)

### 实际使用案例

- 模拟接口响应时间超过阈值 3s 的接口调用

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1639040018709.png)

- 收到超时报警邮件

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1639040083545.png)

- 到 Skywalking UI 中分析链路耗时

![](https://agefades-note.oss-cn-beijing.aliyuncs.com/1639040128431.png)

